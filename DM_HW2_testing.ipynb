{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import csv\n",
    "import scipy.stats as stats\n",
    "from datetime import datetime\n",
    "from matplotlib import gridspec\n",
    "from scipy.signal import argrelmax\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pywt\n",
    "\n",
    "meal1 = pd.read_csv(r\"./Processed_Data/mealData1.csv\", error_bad_lines=False)\n",
    "meal2 = pd.read_csv(r\"./Processed_Data/mealData2.csv\", error_bad_lines=False)\n",
    "meal3 = pd.read_csv(r\"./Processed_Data/mealData3.csv\", error_bad_lines=False)\n",
    "meal4 = pd.read_csv(r\"./Processed_Data/mealData4.csv\", error_bad_lines=False)\n",
    "meal5 = pd.read_csv(r\"./Processed_Data/mealData5.csv\", error_bad_lines=False)\n",
    "\n",
    "meal2.columns = meal1.columns\n",
    "meal3.columns = meal1.columns\n",
    "meal4.columns = meal1.columns\n",
    "meal5.columns = meal1.columns\n",
    "\n",
    "#meal dataframe - all five meal data files appended together\n",
    "meal = pd.concat([meal1, meal2, meal3, meal4, meal5], ignore_index=True, sort = False)\n",
    "\n",
    "no_meal1 = pd.read_csv(r\"./Processed_Data/Nomeal1.csv\", error_bad_lines=False)\n",
    "no_meal2 = pd.read_csv(r\"./Processed_Data/Nomeal2.csv\", error_bad_lines=False)\n",
    "no_meal3 = pd.read_csv(r\"./Processed_Data/Nomeal3.csv\", error_bad_lines=False)\n",
    "no_meal4 = pd.read_csv(r\"./Processed_Data/Nomeal4.csv\", error_bad_lines=False)\n",
    "no_meal5 = pd.read_csv(r\"./Processed_Data/Nomeal5.csv\", error_bad_lines=False)\n",
    "\n",
    "no_meal2.columns = no_meal1.columns\n",
    "no_meal3.columns = no_meal1.columns\n",
    "no_meal4.columns = no_meal1.columns\n",
    "no_meal5.columns = no_meal1.columns\n",
    "\n",
    "#no meal dataframe - all five no meal data files appended together\n",
    "no_meal = pd.concat([no_meal1, no_meal2, no_meal3, no_meal4, no_meal5], ignore_index=True, sort = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Monitoring CGM Trend\n",
    "def extract_cgm_trend(df, result_df):\n",
    "\n",
    "    lunch = [[]]\n",
    "    means = []\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        lunch.append(df.iloc[i])\n",
    "\n",
    "    for i in range(0, len(lunch)):\n",
    "        means.append(df.iloc[i].mean())\n",
    "\n",
    "    countmaster = []\n",
    "\n",
    "    for i in range(0, len(lunch)):\n",
    "        count = 0\n",
    "\n",
    "        for j in df.iloc[i]:\n",
    "            if j < means[i]:    \n",
    "                count += 1\n",
    "\n",
    "        countmaster.append(count)\n",
    "\n",
    "    percentage=[]\n",
    "    for i in countmaster:\n",
    "        percentage.append((i / len(df.iloc[0])) * 100)\n",
    "\n",
    "    result_df['feature_2'] = np.asarray(percentage)\n",
    "    \n",
    "    #return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Accelaration\n",
    "def extract_acceleration(df, result_df):\n",
    "    d=[]\n",
    "    q=[]\n",
    "    f=[]\n",
    "\n",
    "    acc1 = []\n",
    "    acc2 = []\n",
    "    acc3 = []\n",
    "    acc4 = []\n",
    "\n",
    "    for j in range(0, df.shape[0]):\n",
    "        b = df.iloc[j]\n",
    "        d = []\n",
    "        for i in range(len(b)):\n",
    "            if(np.isnan(b[i])):\n",
    "                continue\n",
    "            else:\n",
    "                d.append(b[i])\n",
    "\n",
    "        if(len(d) >= 1):\n",
    "\n",
    "            solar_elevation_angle_1stdev = np.gradient(d)\n",
    "\n",
    "            solar_elevation_angle_2nddev = np.gradient(solar_elevation_angle_1stdev)\n",
    "\n",
    "            q = solar_elevation_angle_2nddev\n",
    "            arr = q[5:10]\n",
    "            acc1.append(np.mean(arr))\n",
    "            arr = q[10:15]\n",
    "            acc2.append(np.mean(arr))\n",
    "            arr = q[15:20]\n",
    "            acc3.append(np.mean(arr))\n",
    "            arr = q[20:25]\n",
    "            acc4.append(np.mean(arr))\n",
    "\n",
    "        else:\n",
    "            acc1.append(0)\n",
    "            acc2.append(0)\n",
    "            acc3.append(0)\n",
    "            acc4.append(0)\n",
    "\n",
    "    result_df['acc1'] = acc1\n",
    "    result_df['acc2'] = acc2\n",
    "    result_df['acc3'] = acc3\n",
    "    result_df['acc4'] = acc4\n",
    "    \n",
    "    #return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Entropy\n",
    "def SampEn(U, m, r):\n",
    "    def _maxdist(x_i, x_j):\n",
    "        result = max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "        return result\n",
    "\n",
    "    def _phi(m):\n",
    "        x = [[U[j] for j in range(i, i + m - 1 + 1)] for i in range(N - m + 1)]\n",
    "        C = 0\n",
    "        for i in range(len(x)):\n",
    "            for j in range(len(x)):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                C += (_maxdist(x[i], x[j]) <= r)\n",
    "        return C\n",
    "    \n",
    "    N = len(U)\n",
    "    if(_phi(m+1) / _phi(m) == 0) :\n",
    "        return 1\n",
    "    return -np.log(float(_phi(m+1)) / float(_phi(m)))\n",
    "\n",
    "def extract_entropy(df, result_df):\n",
    "    no_of_rows = df.shape[0]\n",
    "    no_of_cols = df.shape[1]\n",
    "    \n",
    "    result = df.T\n",
    "    cols = []\n",
    "    for i in range(0, no_of_rows) :\n",
    "        cols.append(str(i))\n",
    "    result.columns = cols \n",
    "    m = 2\n",
    "    \n",
    "    entropy = []\n",
    "    for i in cols :\n",
    "        r = 0.2 * np.std(result[i])\n",
    "        entropy.append(SampEn(result[i], 2, r))\n",
    "\n",
    "    result_df['entropy'] = entropy\n",
    "    #return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract PolyFit\n",
    "def extract_polyfit(df, result_df):\n",
    "   \n",
    "    colid = []\n",
    "    no_of_rows = df.shape[0]\n",
    "    no_of_cols = df.shape[1]\n",
    "\n",
    "    result = df.T\n",
    "    no_of_coefficients = 6\n",
    "    cols = []\n",
    "    for i in range(0, no_of_rows) :\n",
    "        cols.append(str(i))\n",
    "    result.columns = cols\n",
    "    \n",
    "    for i in range(0, no_of_cols) :\n",
    "        colid.append(i + 1)\n",
    "    x = np.array(colid)\n",
    "    fftmatrix = []\n",
    "    \n",
    "    for i in range(0, no_of_rows) :\n",
    "        y = np.array(result[str(i)])\n",
    "        polyres = np.polyfit(x, y, no_of_coefficients-1)  \n",
    "        fftmatrix.append(polyres.tolist())\n",
    "    fftmatrix = np.array(fftmatrix)   \n",
    "  \n",
    "    for i in range(no_of_coefficients) :\n",
    "        result_df['Coefficient'+str(i)] = fftmatrix[:, i]\n",
    "    #return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dwt(df, result_df):\n",
    "    cA, cB = pywt.dwt(df, 'haar')\n",
    "    cA_threshold = pywt.threshold(cA, np.std(cA)/2, mode='soft')\n",
    "    cB_threshold = pywt.threshold(cB, np.std(cB)/2, mode='soft')\n",
    "    \n",
    "    reconstructed_signal = pywt.idwt(cA_threshold, cB_threshold, 'haar')\n",
    "    \n",
    "    feature_dwt_top8 = cA[:,:-8] #sorted in Ascending\n",
    "    #print(feature_dwt_top8.shape)\n",
    "    for i in range(8) :\n",
    "        result_df['DWT'+str(i)] = feature_dwt_top8[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_crossing(df, result_df):\n",
    "    max_vel = []\n",
    "    min_vel = []\n",
    "    zero_cross = []\n",
    "    max_ind = []\n",
    "    min_ind = []\n",
    "    data_list = df.values.tolist()\n",
    "    \n",
    "    x = len(data_list)\n",
    "    y = len(data_list[0])\n",
    "    diff = 5\n",
    "    \n",
    "    for i in range(0, x):\n",
    "        max_v = -1000\n",
    "        min_v = 1000000\n",
    "        c = 0\n",
    "        max_i = 0\n",
    "        min_i = 0\n",
    "        row = data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Feature extraction\n",
    "meal_features = pd.DataFrame()\n",
    "no_meal_features = pd.DataFrame()\n",
    "\n",
    "def extract_features(data, result_df) : \n",
    "    extract_cgm_trend(data, result_df)\n",
    "    extract_acceleration(data, result_df)\n",
    "    extract_entropy(data, result_df)\n",
    "    extract_polyfit(data, result_df)\n",
    "    extract_dwt(data, result_df)\n",
    "\n",
    "extract_features(meal, meal_features)\n",
    "#print(meal_features)\n",
    "extract_features(no_meal, no_meal_features)\n",
    "#print(no_meal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. PCA\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit(meal_features)\n",
    "PCA_mealdata = pca.transform(meal_features)\n",
    "#print(PCA_mealdata)\n",
    "PCA_nomealdata = pca.transform(no_meal_features)\n",
    "#print(PCA_nomealdata)\n",
    "Training_data = np.concatenate((PCA_mealdata, PCA_nomealdata), axis=0)\n",
    "\n",
    "Training_labels = []\n",
    "no_of_mealrows = meal.shape[0]\n",
    "no_of_nomealrows = no_meal.shape[0]\n",
    "for i in range(no_of_mealrows) :\n",
    "    Training_labels.append(1)\n",
    "for i in range(no_of_nomealrows) :\n",
    "    Training_labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input number of files to read: 1\n",
      "Enter file path 1: /home/bhargavi/DM/DM_HW2/Processed_Data/Nomeal3.csv\n",
      "----------------------Processing complete------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/base.py:306: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.21.2 when using version 0.21.3. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "no_of_files = input(\"Input number of files to read: \")\n",
    "file_paths = []\n",
    "for i in range (0, int(no_of_files)):\n",
    "    file_paths.append(input(\"Enter file path \" + str(i + 1) + \": \"))\n",
    "    \n",
    "test_df = pd.read_csv(file_paths[0], error_bad_lines=False)\n",
    "    \n",
    "for i in range(1, len(file_paths)):\n",
    "    sample = pd.read_csv(file_paths[i], error_bad_lines=False)\n",
    "    sample.columns = test_df.columns\n",
    "    test_df = pd.concat([test_df, sample], ignore_index=True, sort = False)\n",
    "    \n",
    "#1. Test data - Feature extraction\n",
    "test_features = pd.DataFrame()\n",
    "\n",
    "def extract_features(test_df, test_features) : \n",
    "    extract_cgm_trend(test_df, test_features)\n",
    "    extract_acceleration(test_df, test_features)\n",
    "    extract_entropy(test_df, test_features)\n",
    "    extract_polyfit(test_df, test_features)\n",
    "    extract_dwt(test_df, test_features)\n",
    "\n",
    "extract_features(test_df, test_features)\n",
    "pca_test_data = pca.transform(test_features)    \n",
    "\n",
    "filename = r\"./Decision_Tree_Model.sav\"\n",
    "clf = pickle.load(open(filename, 'rb'))\n",
    "y_pred = clf.predict(pca_test_data)\n",
    "new_y_pred = []\n",
    "for i in y_pred:\n",
    "    if(i > 0):\n",
    "        new_y_pred.append(1)\n",
    "    else:\n",
    "        new_y_pred.append(0)\n",
    "        \n",
    "with open('./Result_File.csv', 'w', newline='') as file:\n",
    "    wr = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
    "    wr.writerow(new_y_pred)\n",
    "    \n",
    "print(\"----------------------Processing complete------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
